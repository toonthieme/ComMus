---
title: "The 27 Club"
author: "Anthony Thieme"

output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: rows
    self_contained: false
    source: https://github.com/toonthieme/ComMus/blob/main/corpus.Rmd
    theme:
      heading_font:
        google: 
          family: Rajdhani
          wght: 700
      base_font:
        google: Fira Sans
      code_font:
        google: Fira Code
      bg: "#FFFFFF"
      fg: "#212529" 
      primary: "#2b2bee"
      secondary: "#39d7b8"
      success: "#39d7b8"
      danger: "#fa5577"
    
      warning: "#ffb14c"
      info: "#0cc7f1"
---

```{r, echo = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

```{r, setup}
library(tidyverse)

library(tidymodels)
library(plotly)
library(heatmaply)
library(protoclust)
library(cowplot)
library(spotifyr)
library(compmus)
library(ggplot2)
library(ggdendro)
library(heatmaply)

library(htmltools)

library(gridExtra)


```
```{r fetchin_data}
nirvana <- get_playlist_audio_features("","37i9dQZF1DZ06evO3M0Fbi")
hendrix <- get_playlist_audio_features("","37i9dQZF1DZ06evO4cWDcc")
joplin <- get_playlist_audio_features("","37i9dQZF1DZ06evO2Oo4IE")
winehouse <- get_playlist_audio_features("","37i9dQZF1DZ06evO42EWMo")
morrison <- get_playlist_audio_features("","37i9dQZF1DZ06evO19UBIk")

# Combine all tracks into one dataframe and add an artist column
tracks_combined <- bind_rows(
  nirvana %>% mutate(artist = "Nirvana"),
  hendrix %>% mutate(artist = "Jimi Hendrix"),
  joplin %>% mutate(artist = "Janis Joplin"),
  winehouse %>% mutate(artist = "Amy Winehouse"),
  morrison %>% mutate(artist = "Jim Morrison")
)

# Prepare the data by selecting relevant features and renaming appropriately
tracks_prepared <- tracks_combined %>%
  select(track.id, track.name, track.artists, artist, energy, valence, tempo, instrumentalness, danceability, acousticness, liveness, loudness, speechiness, time_signature, mode, key)

slts = get_tidy_audio_analysis("4CeeEOM32jQcH3eN9Q2dGj")

```
# Introduction

Column
-------------------------------------
    
### Introduction

My corpus focuses on the "27 Club," a collective of influential musicians whose lives tragically ended at the age of 27. This group includes iconic figures such as Jimi Hendrix, Janis Joplin, Jim Morrison, Kurt Cobain, and Amy Winehouse. I chose to analyse Spotify playlists dedicated to these artists to explore a diverse array of musical genres, from rock and blues to grunge and soul. My goal is to delve into their unique styles, lyrical content, and the cultural contexts of their times, aiming to uncover both the stark differences and surprising similarities in their music.

In my project, I intend to reflect on the personal struggles, artistic visions, and societal issues these artists addressed through their music. By focusing on audio features like energy, valence, tempo, danceability, and acousticness extracted from Spotify, I aim to dissect the emotional and energetic dimensions of their tracks. A significant part of my analysis involves examining how musical modes (Major vs. Minor) correlate with the mood and intensity of their music, offering insights into the emotional depth conveyed.

I've highlighted the importance of understanding each artist's stylistic evolution, the thematic continuity across varied genres, and the profound emotional resonance in their music. Including both live performances and studio recordings in my analysis might provide deeper insights into their artistry and the raw emotions expressed in live settings.

This corpus is not just a tribute to the talents and legacies of these artists but also a comprehensive examination of their contributions to music and culture. Through detailed analysis—comparing energy and valence, exploring danceability versus acousticness, and examining the impact of musical mode—I seek to offer a nuanced understanding of the "27 Club" members' enduring influence and the complex tapestry of themes that define their work.

For now i have used the "This is .." playlists for each club member, so there are multiple songs. I still need to decide whether i am going to focus on certain songs of these artists (and which ones those will be).

Column
-------------------------------------
    

### Overview of Genres
```{r}
# Assuming you have the following lists of genres for each artist
JH_genres <- c("acid rock", "album rock", "alternative rock", "classic rock", "hard rock", "proto-metal", "psychedelic rock", "rock")
NV_genres <- c("grunge", "permanent wave", "rock")
JJ_genres <- c("album rock", "classic rock", "hard rock", "psychedelic rock", "rock", "singer-songwriter")
AW_genres <- c("british soul", "neo soul")
JM_genres <- c("acid rock", "album rock", "classic rock", "hard rock", "psychedelic rock", "rock")

# Create a vector of genres
genres <- c(JH_genres, NV_genres, JJ_genres, AW_genres, JM_genres)

# Create a vector of artist names, with each name repeated according to the number of genres for each artist
artist_names <- rep(c("Jimi Hendrix", "Kurt Cobain", "Janis Joplin", "Amy Winehouse", "Jim Morrison"), times = c(length(JH_genres), length(NV_genres), length(JJ_genres), length(AW_genres), length(JM_genres)))

# Now, we can create a data frame
data <- data.frame(artist = artist_names, genre = genres)

# Check the data frame
genre_count <- as.data.frame(table(data$genre))
colnames(genre_count) <- c("genre", "count")

# Merge the count back into the original data frame
data_with_count <- merge(data, genre_count, by = "genre")

# Create a bubble plot
# Create a bubble plot with colors
genres_plot <- ggplot(data_with_count, aes(x = artist, y = genre, size = count, color = artist, text = paste("Genre:", genre, "<br>Count:", count))) +
  geom_point(alpha = 0.6) +
  scale_size_continuous(range = c(3, 12)) +
  labs(title = "Interactive Bubble Plot of Artist Genres", x = "Artist", y = "Genre", size = "Genre Count") +
  theme_minimal() +
  theme(legend.position = "bottom")

# Convert to an interactive plot
ggplotly(genres_plot, tooltip = "text")
```

### Overview of Energy x valence
```{r}
# Energy vs. Valence Plot
energy_valence_plot <- ggplot(tracks_prepared, aes(x = valence, y = energy, color = artist)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Energy vs. Valence in Music of the '27 Club'",
    x = "Valence (Musical Positiveness)",
    y = "Energy (Intensity and Activity)",
    color = "Artist"
  ) +
  scale_color_brewer(palette = "Set2") +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.position = "right"
  )
# Convert to an interactive plot
ggplotly(energy_valence_plot, tooltip = "text")
```
### Overview of Tempo x Danceability
```{r}

tempo_dance_plot <- ggplot(tracks_prepared, aes(x = tempo, y = danceability, color = artist)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Tempo vs. Danceability in Music of the '27 Club'",
    x = "Tempo (BPM)",
    y = "Danceability",
    color = "Artist"
  ) +
  scale_color_brewer(palette = "Set2") +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.position = "right"
  )

# Convert to an interactive plot
ggplotly(tempo_dance_plot, tooltip = "text")
```

# Jimi Hendrix {.storyboard data-navmenu=Analysis} 

### About the Artist

Jimi Hendrix, born James Marshall Hendrix on November 27, 1942, in Seattle, Washington, remains one of the most influential and innovative guitarists in the history of popular music. His approach to the electric guitar was revolutionary and pioneering, expanding the instrument's role beyond traditional rhythms and melodies to include feedback, distortion, and previously unexplored effects. Hendrix's technique, characterized by its fluidity, speed, and use of the wah-wah pedal, transformed the sound of rock and roll and inspired countless musicians across various genres.

Hendrix's musical career, though meteoric and brief, left an indelible mark on the music world. He rose to fame in the United States after his performance at the Monterey Pop Festival in 1967, where his audacious guitar playing and dramatic stage antics—such as setting his guitar on fire—cemented his status as a guitar icon. This followed his success in the United Kingdom with the Jimi Hendrix Experience, a trio that produced seminal albums like "Are You Experienced," "Axis: Bold as Love," and "Electric Ladyland."

Hendrix's music was a fusion of blues, rock, R&B, and jazz, showcasing his exceptional talent not only as a guitarist but also as a songwriter and vocalist. His most famous songs, such as "Purple Haze," "Foxy Lady," "The Wind Cries Mary," and his rendition of "The Star-Spangled Banner" at Woodstock in 1969, exemplify his innovative use of the guitar and his profound impact on the sound of contemporary music.

Tragically, Jimi Hendrix's life and career were cut short when he died on September 18, 1970, at the age of 27, joining the infamous "27 Club" of music and cultural figures who died at that age. Despite his brief career, Hendrix's legacy endures. He is celebrated for his contributions to music, particularly his mastery of the electric guitar, which continues to inspire generations of musicians and fans alike. Hendrix was posthumously inducted into the Rock and Roll Hall of Fame in 1992, a testament to his enduring influence and significance in the world of music.

***
Listen here to Jimi Hendrix:

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/artist/776Uo845nYHJpNaStv1Ds4?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

### Chromagrams 

```{r JH-chroma}

watchtower <-
  get_tidy_audio_analysis("2aoo2jlRnM3A0NyLQqMN2f") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

little_wing <-
  get_tidy_audio_analysis("1Eolhana7nKHYpcYpdVcT5") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

purple_haze <-
  get_tidy_audio_analysis("0wJoRiX5K5BxlqZTolB2LD") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)


```


```{r JH-chroma-plots}

watchtower_plot <- 
  watchtower |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "All Along the Watchtower"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


little_wing_plot <- 
  little_wing |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Little Wing"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


purple_haze_plot <- 
  purple_haze |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Purple Haze"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 

plot_grid(watchtower_plot, little_wing_plot, purple_haze_plot, ncol = 1)
```

***
The chromagram for "All Along the Watchtower" reveals a dense and continuous distribution of energy across a wide range of pitches. This suggests a complex texture and a full harmonic structure, which is characteristic of the rich instrumentation typically found in Jimi Hendrix's arrangements. The brighter areas indicate the parts of the track where certain notes are played with higher intensity, possibly during the song's iconic solos or the full-bodied chorus sections. The distribution of energy also reflects the song’s dynamic progression, with peaks that likely correspond to the climactic points of the track.

In the "Little Wing" chromagram, the energy is distributed more sporadically, with clear bursts of brightness. These bursts correspond to moments where specific notes are struck with more force or emphasis, which align with the expressive, nuanced phrasing of Hendrix’s guitar work. The song is known for its emotive guitar playing, and the chromagram captures the ebb and flow of this energy. The variation in color intensity suggests that "Little Wing" has a softer dynamic overall, but with moments of increased intensity that give the song its emotional impact.

The chromagram of "Purple Haze" is marked by areas of high intensity that are more evenly spread out in time, but with a particular concentration on specific pitches. These areas of intensity often represent the song's driving riffs and the anchoring motifs that Hendrix repeats with variations throughout the track. The song's aggressive and upfront style is reflected in the sustained levels of energy in these pitch classes, and the less intense areas likely represent the more subdued vocal sections or breaks between the riffs.

General Observation:

Analyzing the chromagrams of these three songs, we can draw out a common theme of dynamic range and expressiveness, attributes that define Hendrix's musical style. Each chromagram captures the signature fluctuations in intensity that Hendrix brought to his music, whether through his vigorous solos or the subtle nuances of his rhythm playing. The visualization through chromagrams highlights the intricate balance between melody and harmony in his work, showcasing how Hendrix manipulated pitch and dynamics to craft his unique sound. These graphical representations provide a window into the structural and emotional framework of his music, illustrating how Hendrix's approach to the guitar transcended traditional playing to create a rich, multisensory experience.

### Self-Similarity Matrices
#### The Matrices
```{r JH-1.1}
bzt <-
  get_tidy_audio_analysis("2aoo2jlRnM3A0NyLQqMN2f") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") + 
  ggtitle("All Along the Watchtower") # Add your plot title here
```

```{r JH-1.2}
bzt <-
  get_tidy_audio_analysis("1Eolhana7nKHYpcYpdVcT5") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") + 
  ggtitle("Little Wing") # Add your plot title here
```

```{r JH-1.3}
bzt <-
  get_tidy_audio_analysis("0wJoRiX5K5BxlqZTolB2LD") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") +
  ggtitle("Purple Haze") # Add your plot title here

```
***

### Keygram

```{r chordogram_JH}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
 "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

get_tidy_audio_analysis("2aoo2jlRnM3A0NyLQqMN2f") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  compmus_match_pitch_template(key_templates, "aitchison", "manhattan") |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", fill = "Distance") +
  ggtitle("Keygram of All Along The Watchtower")
```

***
    
The keygram of "All Along The Watchtower" visualizes the song's key signatures over time, revealing its harmonic dynamics. Dominant keys, depicted in darker colors, include D# minor and B major, suggesting these tonalities are central to the song's character. The varied shades across the timeline point to a rich song structure, with key changes that likely correspond to different sections such as verses and choruses, highlighting the song’s complex musical architecture.

### Tempo

```{r dance_JH}
get_tidy_audio_analysis('2aoo2jlRnM3A0NyLQqMN2f') |> 
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |> 
  ggplot(aes(x = time, y = bpm, fill = power)) + 
  geom_raster() + 
  scale_fill_viridis_c(guide = 'none') +
  labs(x = 'Time (s)', y = 'Tempo (BPM)') +
  theme_classic()
```

***

The Fourier-based tempogram provides a layered rhythmic analysis of Jimi Hendrix's "All Along the Watchtower," indicating an average tempo of 200 BPM as per Spotify's API. However, this figure contrasts sharply with the track's perceived tempo upon listening and the widely reported tempo of 113 BPM found in various online sources. The discrepancy suggests that the 200 BPM could be a result of the tempogram capturing harmonic rhythm or double-time elements rather than the pulse typically felt by listeners. The tempogram reveals the intricacies of "tempo octaves," with stronger and weaker beats depicted through color intensity, particularly in Hendrix's free-form sections where precise tempo measurement becomes complex. This contrast invites a deeper exploration of the song's tempo, considering the possibility of misinterpretation by automated analysis when faced with the fluidity and nuance of human musical expression.


# Kurt Cobain {.storyboard data-navmenu=Analysis} 

### About the Artist

Kurt Cobain, the lead singer, guitarist, and primary songwriter of Nirvana, remains one of the most iconic figures in the history of rock music. Born on February 20, 1967, in Aberdeen, Washington, Cobain's influence extended far beyond his music, touching on the culture, fashion, and attitudes of an entire generation. Nirvana, formed in 1987, became the flagship band of Generation X and is credited with bringing grunge music to the mainstream with their seminal album, "Nevermind," released in 1991.

Cobain's songwriting was known for its raw emotion, powerful dynamics, and reflective lyrics that delved into personal pain, social alienation, and a disdain for the superficiality of society. Tracks like "Smells Like Teen Spirit," "Come as You Are," and "Heart-Shaped Box" showcased his ability to blend melodic sensibilities with the raw power of punk rock, creating anthems that resonated with a wide audience.

Despite achieving massive commercial success, Cobain was notoriously uncomfortable with fame. He struggled with the expectations placed upon him and was vocal about his issues with the music industry, media, and the misinterpretation of his artistic vision. His personal life, marked by his battle with heroin addiction and his tumultuous relationship with Courtney Love, was heavily scrutinized by the public and media.

Kurt Cobain's life came to a tragic end on April 5, 1994, when he died by suicide at the age of 27. His death marked the end of an era and left a void in the music world that is felt to this day. Cobain's influence on music and culture continues to be celebrated; he is remembered not only for his contributions to grunge and alternative rock but also for his impact on the ethos of a generation. Nirvana's music remains a powerful symbol of the 90s' cultural landscape, embodying the spirit of disillusionment and the quest for authenticity in a commercialized world.

***
Listen here to Kurt Cobain's Nirvana:

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/artist/6olE6TJLqED3rqDCT0FyPh?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

### Chromagrams

```{r KC-chroma}

teen <-
  get_tidy_audio_analysis("4CeeEOM32jQcH3eN9Q2dGj") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

come_as <-
  get_tidy_audio_analysis("2RsAajgo0g7bMCHxwH3Sk0") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

aboutagirl <-
  get_tidy_audio_analysis("2SHTKB8YYlawTGIuJ2b2ok") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)


```


```{r KC-chroma-plots}

teen_plot <- 
  teen |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Smells Like Teen Spirit"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


come_as_plot <- 
  come_as |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Come As You Are"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


aboutagirl_plot <- 
  aboutagirl |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "About A Girl"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 

plot_grid(teen_plot, come_as_plot, aboutagirl_plot, ncol = 1)
```

*** 
The chromagram for "Smells Like Teen Spirit" shows a dense distribution of notes throughout the song, with frequent activity across the range of pitches. The most prominent pitches appear to be around E and F#, which are indicative of the song's key and main power chords. There's a consistent pattern that repeats, reflecting the song's iconic guitar riff. The intensity of colors, particularly in the yellow and green spectrum, suggests a strong presence and repetition of these notes at a high magnitude, likely corresponding to the heavy, distorted guitar that is central to the song's sound. Throughout the timeline, there's a noticeable periodicity, corresponding to the song's structure of verses and choruses. The sections with less color could indicate quieter moments, possibly the "clean" sections of the guitar riff or breaks between the vocal phrases.

For "Come As You Are," the chromagram shows a more concentrated distribution of notes with a particular emphasis on D and E. This aligns with the song's memorable bassline which plays a critical role throughout the track. The continuity of the colors suggests that the song has a steady rhythmic and melodic pattern that does not deviate much, indicative of the song's hypnotic and persistent quality. The colors are not as intense as in "Smells Like Teen Spirit," which could suggest less distortion or dynamic variation in the track. The occasional spikes in magnitude, seen as brief bursts of yellow, could represent the song's chorus or more dynamically intense sections.

The chromagram for "About A Girl" shows a more varied and less dense distribution of notes. There's a visible alternation between pitches, particularly around E, G, and A, which likely represents the song's main chord progression. The variation in color intensity suggests a dynamic play between softer and louder sections, which is characteristic of the song's verse-chorus structure. There's also a visible periodicity, but with more variation than "Come As You Are," implying a greater diversity in the song's sections. The less intense coloration compared to "Smells Like Teen Spirit" could again suggest a lower level of distortion or a more acoustic-driven sound.

Across all three Nirvana songs, there is a distinct pattern of repetition and a strong presence of specific pitches that define each song's key characteristics. The chromagrams reflect the band's grunge style, with "Smells Like Teen Spirit" showing the most intense and dense musical activity, which corresponds to its high-energy, distorted sound. "Come As You Are" presents a more hypnotic and consistent pattern, indicative of its more subdued yet persistent rhythm. "About A Girl" demonstrates dynamic range and a clearer delineation between song sections, suggesting a balance between softer verses and more powerful choruses. Each chromagram visually encapsulates the essence of the song it represents, displaying the distinctiveness of Nirvana's musical style, marked by memorable riffs, impactful dynamics, and a balance between intensity and melody.

### Self-Similarity Matrices
#### The Matrices
```{r NV-1.1}
bzt <-
  get_tidy_audio_analysis("4CeeEOM32jQcH3eN9Q2dGj") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") + 
  ggtitle("Smells Like Teen Spirit") # Add your plot title here
```

```{r NV-1.2}
bzt <-
  get_tidy_audio_analysis("2RsAajgo0g7bMCHxwH3Sk0") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") + 
  ggtitle("Come As You Are") # Add your plot title here
```

```{r NV-1.3}
bzt <-
  get_tidy_audio_analysis("2SHTKB8YYlawTGIuJ2b2ok") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") +
  ggtitle("About A girl") # Add your plot title here

```
***

### Keygram

```{r chordogram_NV}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
 "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

get_tidy_audio_analysis("4CeeEOM32jQcH3eN9Q2dGj") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  compmus_match_pitch_template(key_templates, "aitchison", "manhattan") |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", fill = "Distance") +
  ggtitle("Keygram of Smells Like Teen Spirit")
```

***

This keygram presents the harmonic analysis of Nirvana's "Smells Like Teen Spirit." The vertical axis lists potential keys in both major and minor, while the horizontal axis tracks the progression of the song over time in seconds.

Observing the keygram, it's clear that "Smells Like Teen Spirit" predominantly resides in the key of F minor, as indicated by the consistent dark blue color throughout the majority of the track. This strong presence of F minor suggests that the song does not modulate much, maintaining its key and thus contributing to the track's gritty and powerful sound—characteristics emblematic of the grunge genre.

As the song approaches its end, there's a notable shift to a bright yellow column, which could indicate a significant key change or a moment where harmonic elements are less defined, perhaps during a guitar solo or outro where traditional harmony may be suspended in favor of more expressive or dissonant sounds.

The uniformity of the key throughout suggests a stable harmonic structure, which is a common trait in rock music, often allowing the melodic and rhythmic elements to drive the song's energy and emotive qualities.

### Tempo

```{r dance_KC}
get_tidy_audio_analysis('4CeeEOM32jQcH3eN9Q2dGj') |> 
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |> 
  ggplot(aes(x = time, y = bpm, fill = power)) + 
  geom_raster() + 
  scale_fill_viridis_c(guide = 'none') +
  labs(x = 'Time (s)', y = 'Tempo (BPM)') +
  theme_classic()
```

***

Despite "Smells Like Teen Spirit" having a reported tempo of 117 BPM, this tempogram suggests additional perceived tempo layers. Notably, a prominent line can be seen around the 234 BPM mark. This might represent the song's double-time feel, which some listeners might perceive due to the energetic drumming pattern or the rapid strumming of guitars. The line is consistent and clear, which indicates a strong rhythmic element at this faster tempo.

Furthermore, there are also faint indications of tempo around the 117 BPM mark, aligning with the song's actual tempo. However, these seem less pronounced than the double-time tempo, which could be due to the driving energy of the track that lends itself to a faster perceived beat.

# Janis Joplin {.storyboard data-navmenu=Analysis} 

### About the Artist

Janis Lyn Joplin, born on January 19, 1943, in Port Arthur, Texas, remains one of the most powerful and influential figures in rock and roll history. Her raw, explosive vocal style blended elements of blues, rock, and soul, making her a pioneer of her time and earning her the title of the "Queen of Psychedelic Soul." Joplin's brief but meteoric career in the late 1960s reshaped the boundaries of music and performance for women in the rock genre, leaving an indelible mark that continues to inspire generations of musicians and fans alike.

Joplin's journey to stardom began in her tumultuous teenage years, marked by feelings of alienation and an intense passion for blues music. This passion led her to the vibrant music scene of Austin, Texas, where she first performed folk and blues music. However, it wasn't until she moved to San Francisco and joined the psychedelic rock band Big Brother and the Holding Company that Joplin's career truly took off. Their performance at the Monterey Pop Festival in 1967, highlighted by Joplin's raw and emotionally charged rendition of "Ball and Chain," catapulted her to national fame.

Following her time with Big Brother, Joplin pursued a solo career with her backing groups, first the Kozmic Blues Band and later the Full Tilt Boogie Band. Her first solo effort, "I Got Dem Ol' Kozmic Blues Again Mama!" (1969), showcased a more soulful and bluesy sound. Yet, it was her posthumously released album, "Pearl" (1971), that solidified her legacy. Featuring hits like "Me and Bobby McGee" and "Mercedes Benz," "Pearl" exhibited Joplin's diverse range and depth as a vocalist and songwriter. Tragically, Joplin died of a heroin overdose at the age of 27, just a few weeks before "Pearl" was released.

Beyond her music, Janis Joplin is remembered for her distinctive style, her unabashed expression of emotion, and her refusal to conform to the expectations of women in the music industry and society at large. She was known for her flamboyant, psychedelic wardrobe, her raspy, soulful voice that seemed to carry the weight of her personal struggles, and her electrifying stage presence that captivated audiences.

Joplin's influence extends beyond her musical output; she is seen as a symbol of the 1960s counterculture movement, a pioneer for women in music, and a figure who challenged the norms of her time. Despite her career's brevity, Joplin's contributions to music were profound, earning her posthumous inductions into the Rock and Roll Hall of Fame and the Grammy Lifetime Achievement Award. Her spirit and songs continue to resonate, reminding us of the power of authenticity and the enduring appeal of raw, heartfelt music. Janis Joplin's legacy is not just in the notes she sang but in the barriers she broke down, making the music world a richer, more inclusive place.

***
Listen here to Janis Joplin:

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/artist/4NgfOZCL9Ml67xzM0xzIvC?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

### Chromagrams

```{r JJ-chroma}

mcgee <-
  get_tidy_audio_analysis("1IqFh00G2kvvMm8pRMpehA") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

heart <-
  get_tidy_audio_analysis("1xKQbqQtQWrtQS47fUJBtl") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

crybaby <-
  get_tidy_audio_analysis("3L60Vu9qmY6fg2QroRIxgi") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)


```


```{r JJ-chroma-plots}

mcgee_plot <- 
  mcgee |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Me and Bobby McGee"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


heart_plot <- 
  heart |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Piece of My Heart"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


crybaby_plot <- 
  crybaby |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Cry Baby"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 

plot_grid(mcgee_plot, heart_plot, crybaby_plot, ncol = 1)
```

*** 

The chromagram for "Me and Bobby McGee" shows a repeating pattern, which likely corresponds to the song's chorus and verses. The pitches seem to be concentrated around C, D, and E, with occasional activity in other pitches. This suggests a melody that revolves around these notes, which is common for a song with a strong vocal line. The yellow and green areas indicate sections of the song with higher intensity, possibly when the chorus kicks in or during a powerful vocal part. There is also a noticeable change in the pattern around the halfway mark, which might correspond to a bridge or an instrumental solo where the song's dynamics change.

For "Piece of My Heart," the chromagram shows a dense and varied distribution of notes, with a lot of activity across the range of pitches. This song is known for its raw vocal power and emotional intensity, which could be reflected in the frequent changes in pitch and intensity. There are several spikes of high magnitude throughout the song, likely representing the powerful and emotive delivery that Janis Joplin is famous for. The key notes seem to be around E, F#, and G, which are consistently present throughout the track, suggesting these are central to the song's harmonic structure.

The chromagram for "Cry Baby" shows a somewhat less dense but still varied pattern of notes. The distribution of pitches is less consistent than in "Piece of My Heart," with more sporadic bursts of color, especially in the yellow range. This could be indicative of Joplin's dynamic vocal delivery, with sudden powerful outbursts. The pitches seem to focus around E and F#, with some activity around C and D as well. This may reflect a soulful melody that frequently moves between these notes. The less consistent pattern could also suggest a song with a significant amount of improvisation or variation in its vocal delivery.


The chromagrams of Janis Joplin's songs illustrate the dynamic and soulful style of her music. The dense and varied distribution of notes along with the frequent spikes in magnitude reflect her powerful vocal performances and the emotional intensity of her songs. Each chromagram shows a different pattern, which correlates to the unique structure and delivery of each song. The chromagrams capture the essence of Joplin's music, with its raw energy, emotive power, and the central role of her distinctive vocal style.


### Self-Similarity Matrices
#### The Matrices
```{r JJ-1.1}
bzt <-
  get_tidy_audio_analysis("1IqFh00G2kvvMm8pRMpehA") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") + 
  ggtitle("Me and Bobby McGee") # Add your plot title here
```

```{r JJ-1.2}
bzt <-
  get_tidy_audio_analysis("1xKQbqQtQWrtQS47fUJBtl") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") + 
  ggtitle("Piece of My Heart") # Add your plot title here
```

```{r JJ-1.3}
bzt <-
  get_tidy_audio_analysis("3L60Vu9qmY6fg2QroRIxgi") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") +
  ggtitle("Cry Baby") # Add your plot title here

```
***

### Keygram

```{r chordogram_JJ}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
 "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

get_tidy_audio_analysis("1IqFh00G2kvvMm8pRMpehA") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  compmus_match_pitch_template(key_templates, "aitchison", "manhattan") |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", fill = "Distance") +
  ggtitle("Keygram of Me and Bobby Mcgee")
```

***
    
The range of colors from dark blue to light yellow indicates the prevalence of certain keys at specific times. In the beginning, there is a bright presence in G major, suggesting the song likely starts in this key. The middle section of the song shows more variation in key confidence, with transitions through several keys, reflecting either actual modulations or possibly variations in instrumentation that momentarily emphasize different tonalities.

The yellow streaks throughout indicate moments where the certainty of the key is weaker, which could correlate with musical bridges, solos, or more expressive vocal parts where the harmony becomes ambiguous. Towards the end, there's a return to the darker blues, particularly under G major, suggesting a return to the song's home key for a concluding verse or chorus.

Overall, this keygram suggests that "Me and Bobby McGee" has a clear tonal center with sections of harmonic diversity, creating a rich musical tapestry that adds to the storytelling nature of the song.

### Tempo

```{r dance_JJ}
get_tidy_audio_analysis('1IqFh00G2kvvMm8pRMpehA') |> 
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |> 
  ggplot(aes(x = time, y = bpm, fill = power)) + 
  geom_raster() + 
  scale_fill_viridis_c(guide = 'none') +
  labs(x = 'Time (s)', y = 'Tempo (BPM)') +
  theme_classic()
```

***

Despite the song's reported tempo of 93 BPM, the tempogram shows a consistent line around the 186 BPM mark, which is exactly double the reported tempo. This could suggest that while the song is perceived to be at 93 BPM, the energy and rhythm may give a feel of a faster tempo, possibly because of a double-time feel in the strumming or percussion patterns.

The tempogram doesn't show significant activity at the 93 BPM level, which may be due to the way the song's rhythm interacts with the analysis algorithm. However, the clarity of the line at 186 BPM throughout the song suggests a steady and dominant rhythm.




# Amy Winehouse {.storyboard data-navmenu=Analysis} 

### About the Artist

Amy Winehouse was a British singer-songwriter known for her deep, expressive contralto vocals and her eclectic mix of musical genres including soul, rhythm and blues, jazz, and reggae. Winehouse's music was a reflection of her personal experiences, and she was celebrated for her candid songwriting and powerful performances that often felt like a window into her soul.

Born on September 14, 1983, in London, Winehouse grew up in a family with a love for jazz music, which deeply influenced her artistic development. She attended the Sylvia Young Theatre School before transferring to the BRIT School for Performing Arts and Technology. Winehouse's debut album, "Frank" (2003), was a critical success in the UK and was nominated for the Mercury Prize. The album's title was a nod to her admiration of Frank Sinatra and showcased her abilities not only as a vocalist but also as a songwriter.

However, it was her second album, "Back to Black" (2006), that catapulted her to international fame. The album, produced by Mark Ronson and Salaam Remi, fused soul, jazz, and blues, and was distinguished by its modern yet retro sound. "Back to Black" received widespread critical acclaim and earned Winehouse numerous awards, including five Grammy Awards, making her the first British woman to win five Grammys, including three of the General Field "Big Four" Grammy Awards: Best New Artist, Record of the Year and Song of the Year.

Winehouse's music was notable for its emotional rawness and honesty. Her lyrics often dealt with her personal life, including her tumultuous relationships and struggles with substance abuse. Her signature song, "Rehab," became emblematic of her challenges, with its catchy yet poignant refrain, "They tried to make me go to rehab, I said, 'No, no, no.'"

Despite her commercial success, Winehouse's life was marked by personal struggles. Her frequent battles with substance addiction and mental health issues were heavily publicized in the media, often overshadowing her musical achievements. Her distinctive style, featuring a beehive hairdo and winged eyeliner, made her a muse for fashion designers, but it was her unique voice and talent that left an indelible mark on the music industry.

Winehouse's death on July 23, 2011, at the age of 27, was a tragic end to a career that was both influential and tumultuous. She became a member of the so-called "27 Club," a group of influential musicians who died at the same age. Her legacy, however, endures through her music, which continues to inspire a new generation of artists and music lovers who resonate with her authenticity, artistry, and the timeless quality of her voice and melodies.

Winehouse's influence extended beyond music, with her life and career having been the subject of various documentaries and retrospectives that aim to understand the complexities of her life and the impact of her work. Her posthumous album, "Lioness: Hidden Treasures" (2011), offered a glimpse of her unreleased songs and demos, further solidifying her status as a talented artist taken too soon. Amy Winehouse remains a figure of fascination, remembered for her unmistakable voice, her lyrical genius, and her contribution to modern music.

***
Listen here to Amy Winehouse:

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/artist/6Q192DXotxtaysaqNPy5yR?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

### Chromagrams

```{r AW-chroma}

b2b <-
  get_tidy_audio_analysis("3FAclTFfvUuQYnEsptbK8w") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

valerie <-
  get_tidy_audio_analysis("6nLvaCZFR1wEzW3sIKpsnr") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

rehab <-
  get_tidy_audio_analysis("3N4DI1vuTSX1tz7fa2NQZw") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)


```


```{r AW-chroma-plots}

b2b_plot <- 
  b2b |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Back to Black"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


valerie_plot <- 
  valerie |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Valerie"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


rehab_plot <- 
  rehab |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Rehab"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 

plot_grid(b2b_plot, valerie_plot, rehab_plot, ncol = 1)
```

*** 

The chromagram for "Back to Black" exhibits a pronounced concentration of energy in the lower pitch classes, which suggests a song that is anchored in a lower register, perhaps indicative of a bass line or lower-pitched chords that play a significant role in the song's harmonic structure. The relative consistency of energy across the track may point towards a minimalistic arrangement that does not vary greatly in terms of instrumentation or dynamics throughout the song. This could correlate with the song's somber mood and lyrical theme of mourning a lost relationship. The repeating patterns visible in the chromagram could be reflective of a recurring chord progression that is a hallmark of the song's structure, giving it a sense of cohesiveness and predictability that may resonate with the themes of retrospection and sorrow present in the lyrics.


In contrast, the chromagram for "Valerie" displays a broader distribution of energy across the pitch classes, hinting at a more complex harmonic composition. The patches of higher energy, represented by the warmer colors, intermittently spread across the chromagram, likely correspond to moments in the song where the instrumentation becomes fuller or the vocal delivery more intense, such as during the chorus or bridge. These moments of heightened energy could be capturing the dynamic nature of the song, which, while still deeply emotional, has an upbeat and soulful rhythm that makes it distinct from "Back to Black." The variance in the pattern of energy not only suggests changes in the chord progression but also indicates the song's potential for a call-and-response style between the vocals and backing instruments, a common feature in the soul genre to which this song pays homage.


The chromagram for "Rehab" shows a concentrated band of energy in the mid to lower pitch classes with more distinct spikes in energy than "Back to Black," which could reflect a strong backbeat or a prominent bass line that is characteristic of the song's rhythm and blues influence. The song has a defiant and gritty feel, mirrored in the energy fluctuations that could represent the song's bold brass sections and the rhythmic emphasis that drives the track. The variations in the energy pattern suggest a more complex musical structure than "Back to Black," with potential key changes or varied instrumental sections that add to the song's vigorous and rebellious spirit. The chromagram captures the song's dynamic shifts, which echo the narrative of personal struggle and resistance in the face of external pressures, a central theme of the lyrics.


Looking across all three songs, it's evident that Amy Winehouse's music encompasses a range of emotional and harmonic complexity. The chromagrams reflect not only the differences in mood and thematic content between the songs but also the shared qualities that define her musical style. Each song has its own distinctive pattern of energy across the pitch classes, reflecting their unique chord progressions, melodic lines, and overall arrangements. These visual patterns underscore Winehouse's ability to convey a wide array of emotions through her music—from the melancholy of "Back to Black" to the spirited soulfulness of "Valerie," and the bold defiance of "Rehab." The chromagrams, in essence, offer a visual counterpart to the auditory experience of Winehouse's music, highlighting the depth and diversity of her songwriting and compositional skills. Her songs manage to weave intricate musical structures with emotionally charged performances, creating a lasting impact that is both visually and sonically apparent.

### Self-Similarity Matrices
#### The Matrices
```{r AW-1.1}
bzt <-
  get_tidy_audio_analysis("3FAclTFfvUuQYnEsptbK8w") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") + 
  ggtitle("Back to Black") # Add your plot title here
```

```{r AW-1.2}
bzt <-
  get_tidy_audio_analysis("6nLvaCZFR1wEzW3sIKpsnr") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") + 
  ggtitle("Valerie") # Add your plot title here
```

```{r AW-1.3}
bzt <-
  get_tidy_audio_analysis("3N4DI1vuTSX1tz7fa2NQZw") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") +
  ggtitle("Rehab") # Add your plot title here

```
***

### Keygram

```{r chordogram_AW}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
 "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

get_tidy_audio_analysis("3FAclTFfvUuQYnEsptbK8w") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  compmus_match_pitch_template(key_templates, "aitchison", "manhattan") |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", fill = "Distance") +
  ggtitle("Keygram of Back to Black")
```

***
    
Darker shades represent a stronger presence of the key at that time, while lighter shades suggest less prominence. From the keygram, it appears that "Back to Black" predominantly revolves around D minor, as indicated by the consistent dark blue swath across the chart. This key is known for its somber and introspective mood, which aligns with the emotional depth of the song.

There is also a noticeable light yellow area towards the end of the song, which could indicate a shift in the instrumentation or vocal harmony that momentarily brings other keys into focus, adding to the song’s complex musical texture and emotional impact.

The use of a single dominant key, as suggested by this keygram, is typical for songs in the soul genre, which often rely on a strong emotional delivery and a consistent harmonic foundation to support it.

### Tempo

```{r dance_AW}
get_tidy_audio_analysis('3FAclTFfvUuQYnEsptbK8w') |> 
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |> 
  ggplot(aes(x = time, y = bpm, fill = power)) + 
  geom_raster() + 
  scale_fill_viridis_c(guide = 'none') +
  labs(x = 'Time (s)', y = 'Tempo (BPM)') +
  theme_classic()
```

***

The song's reported tempo is 123 BPM, but this tempogram reveals a consistent presence of rhythmic elements around the 246 BPM range, indicating that while the song may fundamentally have a tempo of 123 BPM, there's a perceivable double-time rhythm at play. This could be due to elements within the song that create a feel of faster tempo, like a hi-hat played in double time or other percussive embellishments.

Scattered spots around 62 BPM may suggest instances where the rhythm feels like it's at half-time, emphasizing the moody and weighty atmosphere of the track. The presence of rhythmic components at these various tempo octaves contributes to the song's dynamic character, which is described as having high energy and being somewhat danceable despite its moody tone.

The 4/4 time signature, typical for many popular songs, underpins this structure, providing a steady base for Amy Winehouse's soulful and emotive delivery throughout the track's four-minute duration.



# Jim Morrison {.storyboard data-navmenu=Analysis} 

### About the Artist
Jim Morrison, the enigmatic frontman of The Doors, was an iconic figure of the 1960s rock scene, known for his poetic lyrics, baritone voice, and wild personality. Born on December 8, 1943, in Melbourne, Florida, Morrison was not only a singer but also a poet, songwriter, and filmmaker. His charisma and onstage persona made him one of the most legendary and provocative figures in the history of rock music.

Morrison co-founded The Doors in 1965 in Los Angeles, California, with keyboardist Ray Manzarek, guitarist Robby Krieger, and drummer John Densmore. The band's name was inspired by Aldous Huxley's book "The Doors of Perception," which in turn was a reference to a William Blake quotation: "If the doors of perception were cleansed, everything would appear to man as it is, infinite."

The Doors quickly gained fame with hits like "Light My Fire," "Break On Through (To the Other Side)," and "The End," showcasing Morrison's deep, bellowing voice and poetic lyrics. The band's music was a fusion of rock, blues, and psychedelia, and it often explored themes of death, love, rebellion, and the human psyche. Morrison's songwriting was heavily influenced by literary figures such as Arthur Rimbaud, Friedrich Nietzsche, and Jack Kerouac, and he was known for often improvising spoken word passages during live performances.

Morrison's public image was marked by a series of tumultuous events, including on-stage arrests and his infamous indecency arrest in Miami, which solidified his status as a counterculture icon. His unpredictable behavior and substance abuse were a constant source of tension within the band and with law enforcement, contributing to his mystique as well as his eventual downfall.

Despite his troubled life, Morrison's impact on rock music and popular culture is undeniable. He was one of the earliest rock stars to embody the archetype of the tortured artist, grappling with existential angst and the burdens of fame. His lyrics and poetry are still studied for their depth and intensity, and The Doors' music remains influential, covered and celebrated by artists across various genres.

Jim Morrison died in Paris on July 3, 1971, at the age of 27, under circumstances that remain somewhat mysterious, adding to the mythos surrounding his life and career. As with other members of the infamous "27 Club," his legacy has been romanticized and remains a subject of fascination, symbolizing the rebellion and excesses of rock and roll. Morrison's grave at Père Lachaise Cemetery in Paris is a pilgrimage site for fans, and his spirit continues to captivate those who are drawn to his artistry and the era he came to define.

***
Listen here to Jim Morrison's The Doors:

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/artist/22WZ7M8sxp5THdruNY3gXt?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

### Chromagrams

```{r JM-chroma}

people <-
  get_tidy_audio_analysis("1Jmqubf9kGkWeYQXQKImL5") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

riders <-
  get_tidy_audio_analysis("14XWXWv5FoCbFzLksawpEe") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

breakon <-
  get_tidy_audio_analysis("6ToM0uwxtPKo9CMpbPGYvM") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)


```


```{r JM-chroma-plots}

people_plot <- 
  people |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "People Are Strange"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


riders_plot <- 
  riders |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Riders on the Storm"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 


breakon_plot <- 
  breakon |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +

  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Break on Through (To the Other Side)"
  ) +
  theme_minimal() +
  scale_fill_viridis_c() 

plot_grid(people_plot, riders_plot, breakon_plot, ncol = 1)
```

***

The chromagram for "People Are Strange" reveals a varied distribution of pitch classes with notable activity in the mid-range frequencies. The song’s harmonic content appears to fluctuate, which may correspond to its distinctive melody and changes in harmonic progression. There's a sense of unease and peculiarity, much like the song's title and lyrics suggest. The repeated patterns indicate a structured musical form but with enough variation to keep the listener engaged. The chromagram shows that the song does not rely heavily on the lower bass frequencies, which might be indicative of a lighter instrumentation or a focus on mid-range instruments like the guitar and the organ, consistent with the band’s style.

"Riders on the Storm" presents a chromagram with a long duration, as it is a longer track, and displays a relatively consistent pattern of intensity across the pitch classes. This consistency might reflect the song’s steady, driving beat that emulates the sound of a thunderstorm, which is a central theme of the track. The energy seems to be spread across both lower and higher pitch classes, suggesting a rich harmonic texture that includes both bass and melodic elements. The extended sections of similar coloration could indicate repetitive or hypnotic sections within the song, aligning with its atmospheric and immersive nature. There's a sense of a journey, both musically and lyrically, which is captured in the sustained energy throughout the piece.

For "Break on Through (To the Other Side)," the chromagram shows a high concentration of energy in certain pitch classes at various points throughout the song. These could be indicative of the song's iconic, punchy keyboard riffs and the strong, rhythmic bass lines that drive the track forward. The intense bursts of color suggest a more energetic and forceful composition, aligning with the song's theme of breaking free from constraints. The chromagram captures the essence of the song's dynamic shifts and the psychedelic rock influence with its sharp contrasts in harmonic content. 

The chromagrams for The Doors' songs illustrate the band's diverse musical approach, with each track offering a different array of harmonic complexities and emotional tones. "People Are Strange" shows the band's ability to weave an air of mystery and introspection into their music, "Riders on the Storm" captures a more meditative and immersive experience, and "Break on Through (To the Other Side)" reflects the energy and rebellious spirit of the band. The chromatic visualization of these songs underscores The Doors' skill in balancing repetitive, hypnotic elements with varied and intense musical moments to create songs that are both catchy and deeply evocative. Each song's chromagram, therefore, provides a visual map of its musical journey, showcasing the distinct and rich tapestry of sounds that define The Doors' enduring legacy in rock music.

### Self-Similarity Matrices
#### The Matrices
```{r JM-1.1}
bzt <-
  get_tidy_audio_analysis("1Jmqubf9kGkWeYQXQKImL5") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") + 
  ggtitle("People Are Strange") # Add your plot title here
```

```{r JM-1.2}
bzt <-
  get_tidy_audio_analysis("14XWXWv5FoCbFzLksawpEe") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") + 
  ggtitle("Riders on the Storm") # Add your plot title here
```

```{r JM-1.3}
bzt <-
  get_tidy_audio_analysis("6ToM0uwxtPKo9CMpbPGYvM") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
 theme_classic() + 
  labs(x = "", y = "") +
  ggtitle("Break on Through (To the Other Side)") # Add your plot title here

```
***

### Keygram

```{r chordogram_JM}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
 "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

get_tidy_audio_analysis("1Jmqubf9kGkWeYQXQKImL5") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  compmus_match_pitch_template(key_templates, "aitchison", "manhattan") |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", fill = "Distance") +
  ggtitle("Keygram of People are Strange")
```

***
    
The darker the color, the more prominent the key is at that moment in the song. This keygram shows a strong presence in a few keys, with E minor being quite dominant for a significant portion of the track, as indicated by the dark blue color. E minor is often associated with a melancholic and mysterious sound, which is fitting for the song's title and themes.

Towards the end of the song, there is a noticeable transition to lighter colors, suggesting a possible key change or a section with less defined tonality. This could correspond to a bridge or instrumental solo that moves away from the main key, adding to the song's dynamic and enigmatic quality.

Overall, the keygram suggests that "People are Strange" has a rich harmonic structure with a few key changes, enhancing the song's captivating and somewhat eerie mood.


### Tempo

```{r dance_JM}
get_tidy_audio_analysis('1Jmqubf9kGkWeYQXQKImL5') |> 
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |> 
  ggplot(aes(x = time, y = bpm, fill = power)) + 
  geom_raster() + 
  scale_fill_viridis_c(guide = 'none') +
  labs(x = 'Time (s)', y = 'Tempo (BPM)') +
  theme_classic()
```

***


The tempogram shows the tempo fluctuations for The Doors' "People Are Strange." The horizontal axis represents time in seconds, covering the song's duration of 2 minutes and 10 seconds, while the vertical axis represents tempo in BPM (beats per minute).

A visible line running horizontally around the 120 BPM range suggests that the song's primary tempo is maintained consistently throughout the song, which aligns with the reported tempo of 119 BPM. The line's clarity throughout the tempogram indicates that this tempo is prominent and steady, which supports the song's danceable quality and time signature of 4 beats per bar.

There doesn't appear to be a strong presence at the 60 BPM range, which would represent a half-time feel. Similarly, while the song can be perceived at a double-time tempo of 238 BPM, the tempogram does not show a distinct line at that level. This might be due to the song's rhythmic elements being more naturally aligned with the 119 BPM tempo.

The consistency of the main tempo line across the tempogram also supports the song's average energy level, providing a stable rhythmic backbone that might be accentuated by the danceable groove of the track. The minor key of E likely adds to the song's unique atmosphere, overlaying the steady rhythm with a distinctive melodic character.



Playlist Comparison
==========================
Column
-------------------------------------
### Comparison

```{r}
library(ggplot2)
library(gridExtra)


# Load necessary libraries
library(dplyr)

# Define function to fetch playlist audio features and add audio analysis
get_and_analyze_playlist <- function(playlist_id, playlist_name) {
  playlist_features <- get_playlist_audio_features("thesoundsofspotify", playlist_id) %>%
    slice(1:50) %>%
    add_audio_analysis() %>%
    mutate(playlist_name = playlist_name)
  return(playlist_features)
}

# Function to extract keys and modes from audio features
extract_keys_and_modes <- function(audio_features) {
  # Convert integer key values to character key names
  key_names <- c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")
  keys <- factor(key_names[audio_features$key])
  modes <- factor(ifelse(audio_features$mode == 0, "Minor", "Major"))
  return(list(keys = keys, modes = modes))
}

# Fetch audio features and analyze playlists for Artists
nirvana1 <- get_and_analyze_playlist("37i9dQZF1DZ06evO3M0Fbi", "Kurt Cobain")
hendrix1 <- get_and_analyze_playlist("37i9dQZF1DZ06evO4cWDcc", "Jimi Hendrix")
joplin1 <- get_and_analyze_playlist("37i9dQZF1DZ06evO2Oo4IE", "Janis Joplin")
winehouse1 <- get_and_analyze_playlist("37i9dQZF1DZ06evO42EWMo", "Amy Winehouse")
morrison1 <- get_and_analyze_playlist("37i9dQZF1DZ06evO19UBIk", "Jim Morrison")

# Extract keys and modes for each playlist
nirvana_keys_modes <- extract_keys_and_modes(nirvana1)
hendrix_keys_modes <- extract_keys_and_modes(hendrix1)
joplin_keys_modes <- extract_keys_and_modes(joplin1)
winehouse_keys_modes <- extract_keys_and_modes(winehouse1)
morrison_keys_modes <- extract_keys_and_modes(morrison1)


# Define all 12 possible keys
all_keys <- factor(c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"))

# Make sure all playlists have the same number of keys
num_keys <- max(
  length(nirvana_keys_modes$keys),
  length(hendrix_keys_modes$keys),
  length(joplin_keys_modes$keys),
  length(winehouse_keys_modes$keys),
  length(morrison_keys_modes$keys)
)

# Replicate keys and modes to match the max number of keys
nirvana_keys <- rep(nirvana_keys_modes$keys, length.out = num_keys)
nirvana_modes <- rep(nirvana_keys_modes$modes, length.out = num_keys)
hendrix_keys <- rep(hendrix_keys_modes$keys, length.out = num_keys)
hendrix_modes <- rep(hendrix_keys_modes$modes, length.out = num_keys)
joplin_keys <- rep(joplin_keys_modes$keys, length.out = num_keys)
joplin_modes <- rep(joplin_keys_modes$modes, length.out = num_keys)
winehouse_keys <- rep(winehouse_keys_modes$keys, length.out = num_keys)
winehouse_modes <- rep(winehouse_keys_modes$modes, length.out = num_keys)
morrison_keys <- rep(morrison_keys_modes$keys, length.out = num_keys)
morrison_modes <- rep(morrison_keys_modes$modes, length.out = num_keys)

# Create data frames for each playlist
nirvana_df <- data.frame(Key = factor(nirvana_keys, levels = all_keys), Mode = nirvana_modes, Playlist = "Nirvana")
hendrix_df <- data.frame(Key = factor(hendrix_keys, levels = all_keys), Mode = hendrix_modes, Playlist = "Hendrix")
joplin_df <- data.frame(Key = factor(joplin_keys, levels = all_keys), Mode = joplin_modes, Playlist = "Joplin")
winehouse_df <- data.frame(Key = factor(winehouse_keys, levels = all_keys), Mode = winehouse_modes, Playlist = "Winehouse")
morrison_df <- data.frame(Key = factor(morrison_keys, levels = all_keys), Mode = morrison_modes, Playlist = "Morrison")

# Combine data frames
combined_df <- rbind(nirvana_df, hendrix_df, joplin_df, winehouse_df, morrison_df)
# Plot histograms for major keys
major_df <- combined_df %>% filter(Mode == "Major")
plot_major <- ggplot(major_df, aes(x = Key, fill = Playlist)) +
  geom_bar(stat = "count", position = "dodge", width = 0.7) +
  labs(
    x = "Key",
    y = "Frequency",
    title = "Histogram of Major Keys for the Artists"
  ) +
  scale_fill_manual(values = c("Nirvana" = "skyblue","Hendrix" = "salmon","Joplin" = "lightgreen","Winehouse" = "yellow","Morrison" = "lavender")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot histograms for minor keys
minor_df <- combined_df %>% filter(Mode == "Minor")
plot_minor <- ggplot(minor_df, aes(x = Key, fill = Playlist)) +
  geom_bar(stat = "count", position = "dodge", width = 0.7) +
  labs(
    x = "Key",
    y = "Frequency",
    title = "Histogram of Minor Keys for the artists"
  ) +
  scale_fill_manual(values = c("Nirvana" = "skyblue","Hendrix" = "salmon","Joplin" = "lightgreen","Winehouse" = "yellow","Morrison" = "lavender")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Assuming combined_df contains the combined data for both playlists
# Assuming combined_df contains the combined data for both playlists

# Count the number of major and minor keys in each playlist
major_minor_count_per_playlist <- combined_df %>%
  group_by(Playlist, Mode) %>%
  summarise(Count = n()) %>%
  pivot_wider(names_from = Mode, values_from = Count, values_fill = list(Count = 0))


# Arrange plots in a grid
grid <- grid.arrange(plot_major, plot_minor, ncol = 1)


```
Column
-------------------------------------

### **Major Key Histogram Story:**

In a colorful display of musical preference, the Histogram of Major Keys charts a tale of harmony and style across the sonic landscapes curated by five legendary artists. The bar for the key of C towers like a lighthouse over the Hendrix playlist, signaling a fondness for the pure and unadulterated sound that a major key rooted in C provides. This preference sets Hendrix's selections apart, revealing a possible penchant for the bright and straightforward clarity that the key of C offers in the realm of rock.

As we move through the chromatic spectrum, the key of G appears to play a symphony of diversity, with almost equal representation across all artists, suggesting it as a universal favorite - a key that resonates with the strings of various musical souls.

The presence of the key of A# in the Joplin playlist is like a rare flower in an eclectic garden, blooming brightly but infrequently, hinting at a taste for the unconventional or perhaps a signature song that frequents the playlist.

Column
-------------------------------------
###**Minor Key Histogram Story:**

Descending into the depths of the minor keys, the Histogram of Minor Keys sings a ballad of introspection and emotional depth. The playlists take on a more complex narrative where the key of C# underlines the Nirvana playlist with a heavy stroke. This suggests an embrace of the melancholic and introspective nature that minor keys often evoke, possibly reflecting the grunge era's gritty and raw aesthetic.

Joplin's playlist, with a significant presence in the key of F, whispers a story of soulful sorrow, a testament to bluesy influences and earthy tones that minor keys carry with their flat thirds and sevenths.

Morrison's and Winehouse's selections scatter across the keys, suggesting a tapestry woven from a broad palette of moods and stories, capturing the essence of their varied musical journeys.

Hendrix, while known for his electric major key riffs, also dips his brush into the somber hues of minor keys, adding shades of complexity and depth to his collection.

In summary, the major keys represent the artists' brighter, more vibrant selections, while the minor keys paint the playlists with strokes of introspection and nuanced emotion. These histograms not only showcase musical preferences but also act as windows into the artistic souls of these musicians, with each bar a note in their larger musical narrative.


Time Signature in different playlists
==========================
Column
-------------------------------------
### Time Signature

```{r }

# Load necessary libraries
library(ggplot2)
library(dplyr)

combined_df <- rbind(
  data.frame(Playlist = "Kurt Cobain", time_signature = nirvana$time_signature),
  data.frame(Playlist = "Jimi Hendrix", time_signature = hendrix$time_signature),
  data.frame(Playlist = "Janis Joplin", time_signature = joplin$time_signature),
  data.frame(Playlist = "Amy Winehouse", time_signature = winehouse$time_signature),
  data.frame(Playlist = "Jim Morrison", time_signature = morrison$time_signature)
)

# Create histogram to compare time signatures across playlists
ggplot(combined_df, aes(x = time_signature, fill = Playlist)) +
  geom_histogram(binwidth = 0.5, position = "dodge") +
  labs(
    title = "Comparison of Time Signatures Across Artists",
    x = "Time Signature",
    y = "Count"
  ) +
  scale_fill_manual(values = c("skyblue", "salmon", "green", "purple", "lavender")) +
  theme_minimal()

```
Column
-------------------------------------
### Time Signature explanation
The graph titled "Comparison of Time Signatures Across Playlists" provides a visual narrative of rhythmic variety in the music collections of Amy Winehouse, Janis Joplin, Jim Morrison, Jimi Hendrix, and Kurt Cobain.

In this story, the 4/4 time signature stands as the grand monolith—a universal heartbeat in popular music, dominant and central to the rhythmic structure of the songs across all the featured artists' playlists. The bars in vibrant colors reaching upwards in a near-uniform height for the 4/4 time signature signify its foundational role in modern music, often considered the most common and accessible time signature for listeners and musicians alike.

The absence of bars at the 0 and 2 markers indicates a near-nonexistent use of more uncommon time signatures such as 1/4 or 2/4 in these collections, hinting at a shared stylistic choice for stability and groove that a 4/4 time signature offers.

The solitary bars for Amy Winehouse and Kurt Cobain at the extreme right tell tales of rare forays into less conventional time signatures, perhaps a waltz with the unusual for artistic expression or simply an outlier track that breaks from the norm.

This histogram doesn't just compare counts; it underscores the rhythmical consistency that underscores the oeuvre of these music legends, illustrating how, despite their unique styles and voices, they often built their songs upon the same temporal foundation—a testament to the unifying power of the 4/4 time signature in the language of music.

Dendogram
==========================
Column
-------------------------------------
### Dendogram plot
```{r}
library(tidyr)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(purrr)
library(compmus)

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  
nirvanaa <-
  get_playlist_audio_features("thesoundsofspotify", "37i9dQZF1DZ06evO3M0Fbi") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))|>
  arrange(desc(track.popularity)) |>
  slice_head(n = 10)

hendrixx <-
  get_playlist_audio_features("thesoundsofspotify", "37i9dQZF1DZ06evO4cWDcc") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))|>
  arrange(desc(track.popularity)) |>
  slice_head(n = 10)

joplinn <-
  get_playlist_audio_features("thesoundsofspotify", "37i9dQZF1DZ06evO2Oo4IE") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))|>
  arrange(desc(track.popularity)) |>
  slice_head(n = 10)

winehousee <-
  get_playlist_audio_features("thesoundsofspotify", "37i9dQZF1DZ06evO42EWMo") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))|>
  arrange(desc(track.popularity)) |>
  slice_head(n = 10)

morrisonn <-
  get_playlist_audio_features("thesoundsofspotify", "37i9dQZF1DZ06evO19UBIk") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))|>
  arrange(desc(track.popularity)) |>
  slice_head(n = 10)

halloween <- bind_rows(nirvanaa, hendrixx, joplinn, winehousee, morrisonn)

halloween_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = halloween
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(halloween |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")


```

```{r}
# Assuming 'playlist_name' is a column in 'halloween', add it to 'halloween_juice'
halloween_juice <- halloween_juice %>%
  mutate(playlist_name = halloween$playlist_name)

halloween_dist <- dist(halloween_juice, method = "euclidean")
```

```{r}
library(dendextend)

# Create a dendrogram object
dend <- as.dendrogram(hclust(halloween_dist, method = "complete"))

# Color branches by playlist_name
dend <- color_branches(dend, k = 5)  # The k parameter should be set to the number of playlists

# Use ggdendrogram to plot the colored dendrogram
ggdendrogram(dend, show_labels = TRUE) +
  theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
    legend.position = "right"
  )
```
Column
-------------------------------------
### Dendogram Explained
It's a delightful surprise to unearth hidden connections in music through data analysis. For instance, imagine telling a room full of rock and soul enthusiasts that Nirvana's grunge anthem 'Smells Like Teen Spirit' is a data-doppelgänger to Amy Winehouse's soulful 'Valerie'—eyebrows would rise! Yet, when we delve into the numbers and patterns within the music, this unlikely pairing emerges. It’s like finding out that two radically different ingredients can create an unexpectedly harmonious dish.

This clustering is particularly fascinating because it's not immediately evident when listening. The raw emotional punch of 'Smiles Like Teen Spirit' and the poignant yet upbeat vibe of 'Valerie' seem worlds apart. But beneath the surface, they share a skeleton of sonic qualities that binds them in the analytical realm. It's these subtleties that might escape the casual listener but are captured in the meticulous net of musical metrics.

Moreover, when we zoom out and see clusters of songs from different artists snuggling up close together, it paints a picture of genre-blurring camaraderie. Songs from Nirvana, Janis Joplin, The Doors, and Amy Winehouse forming a mosaic of melodies, suggests that the artists, often pigeonholed into distinct genres, have more in common than genre labels would suggest. It's an eloquent reminder of music's universal language, transcending perceived boundaries and uniting diverse sounds under a single, harmonious roof.

Data reveals the threads that weave through music, connecting artists and songs in a tapestry far more intricate than the binary of rock or soul, grunge or blues. It's an exquisite symphony of numbers and notes where, against all odds, Kurt Cobain and Amy Winehouse share a silent duet, nodding to the complexity and interconnectedness of musical expression.


Conclusion
==========================

### Conclusion

The corpus study on "The 27 Club" unravels a rich tapestry of auditory brilliance that transcends the tragic narrative of artists united by their premature departure at the age of 27. Through the analytical lens of Spotify playlists, the study navigates the confluence of Jimi Hendrix's revolutionary guitar solos, Janis Joplin's soul-stirring vocal power, Jim Morrison's poetic musicality, Kurt Cobain's raw grunge anthems, and Amy Winehouse's emotive rhythm and blues.

The chromatic and rhythmic analyses illuminate the distinctive characteristics that each artist brought to the musical landscape. From the vibrant major key melodies to the soulful minor key harmonies, the histograms reveal a diverse but discernible preference for certain tonalities, reflecting the unique artistry and emotional landscapes of the club members. The ubiquitous 4/4 time signature across the playlists speaks to a shared structural foundation, a rhythmic heartbeat that is both grounding and universally accessible.

Through the heatmaps and self-similarity matrices, the study uncovers unexpected sonic kinship, such as the parallel between the energetic crescendos of "Smells Like Teen Spirit" and the soulful grooves of "Valerie." These surprising affiliations underscore the universal qualities of music that cross genre boundaries, suggesting a deeper level of artistic and emotional resonance.

The Fourier-based tempograms and tempo analyses challenge our perception of musical pace, revealing the complexities of rhythm and how it is perceived, potentially contrasting the analytical data with the human experience of the music. Such disparities invite a contemplation on the nature of musical interpretation—algorithmic versus experiential.

In conclusion, the members of "The 27 Club," through their diverse and groundbreaking contributions, demonstrate the myriad ways in which music can be constructed, perceived, and appreciated. While their careers were marked by individual struggles and their lives poignantly brief, the enduring influence of their music is a testament to their exceptional talents. This study not only celebrates their legacies but also contributes to a deeper understanding of their artistic expressions, capturing the eternal echoes of their voices in the world of music.

In the end, "The 27 Club" is not solely defined by loss but illuminated by the indelible impact of its members, whose musical echoes continue to resonate through the ages, challenging, inspiring, and redefining the contours of musical expression.



